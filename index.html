<!DOCTYPE html><html lang="en"><head>
	<meta name="generator" content="Hugo 0.134.2">
    <meta charset="utf-8">
<title>Decentralized Multi-Drone Coordination for Wildlife Video Acquisition</title>
<meta name="description" content="ACSOS 2024 main-track paper presentation">
<meta name="author" content="Danilo Pianini">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><link rel="stylesheet" href="/slides-2024-acsos-imageonomics/reveal-js/dist/reset.css">
<link rel="stylesheet" href="/slides-2024-acsos-imageonomics/reveal-js/dist/reveal.css">
  <link rel="stylesheet" href="/slides-2024-acsos-imageonomics/css/custom-theme.min.44bce6737f879e6855148ed296b9fbe60b92c7fa33d0998533fd535047a2b4d2.css" id="theme"><link rel="stylesheet" href="/slides-2024-acsos-imageonomics/highlight-js/solarized-dark.min.css">
<link rel="stylesheet" href="https://gitcdn.link/repo/DanySK/css-blur-animation/master/blur.css">
<link href="https://fonts.googleapis.com/css?family=Roboto Mono" rel="stylesheet">
<link href="https://fonts.googleapis.com/css?family=Oxygen Mono" rel="stylesheet">
<link href="https://fonts.googleapis.com/css?family=Ubuntu Mono" rel="stylesheet">
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-4bw+/aepP/YC94hEpVNVgiZdgIC5+VKNBQNGCHeKRQN+PtmoHDEXuppvnDJzQIu9" crossorigin="anonymous">
<script src="https://kit.fontawesome.com/81ac037be0.js" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/arrow-line/dist/arrow-line.min.js"></script>
<script type="text/javascript" src="https://unpkg.com/qr-code-styling@1.5.0/lib/qr-code-styling.js"></script>

  </head>
  <body>
    
    <div class="reveal">
      <div class="slides">
  

    
<section data-noprocess="" data-shortcode-slide="" data-background-image="https://danysk.github.io/slides-2024-acsos-imageonomics/zebras.jpg">
  
<div style="
position: fixed;
/* bottom: 0;
right: 0; */
font-size: .3em;
top: 800px;
left: 50%;
transform: translate(-50%, -50%);
">background image adapted from: Diego Delso, delso.photo, License CC BY-SA</div>
<h1 id="decentralized-multi-drone-coordination-for-wildlife-video-acquisition">Decentralized Multi-Drone Coordination for Wildlife Video Acquisition</h1>
<p><a href="mailto:denys.grushchak@studio.unibo.it">Denys Grushchak</a> <i class="fa-solid fa-computer"></i>,
<a href="mailto:kline.377@osu.edu">Jenna Kline</a> <i class="fa-solid fa-horse"></i>,
<a href="mailto:danilo.pianini@unibo.it">Danilo Pianini</a> <i class="fa-solid fa-computer"></i>, <br>
<a href="mailto:nicolas.farabegoli@unibo.it">Nicolas Farabegoli</a> <i class="fa-solid fa-computer"></i>,
<a href="mailto:gianluca.aguzzi@unibo.it">Gianluca Aguzzi</a> <i class="fa-solid fa-computer"></i>,
<a href="mailto:m.baiardi@unibo.it">Martina Baiardi</a> <i class="fa-solid fa-computer"></i>,
and
<a href="cstewart@cse.ohio-state.edu">Christopher Stewart</a> <i class="fa-solid fa-horse"></i></p>

<div class="container w-100 m-0 p-0">
    <div class="row "><div class="col "><div style="text-align: center; width: 100%;">
<img src="example-background.svg" style="width: 50%">
</div>
<p><i class="fa-solid fa-computer"></i> Department of Computer Science and Engineering, University of Bologna, Cesena (FC), Italy</p>
</div>
<div class="col "><div style="text-align: center; width: 100%;">
<img src="osu.svg" style="width: 50%">
</div>
<p><i class="fa-solid fa-horse"></i> Computer Science and Engineering Department, The Ohio State University, Columbus (OH), USA</p>
</div>
</div>
</div>
</section>

<section data-noprocess="" data-shortcode-slide="" data-background-image="https://danysk.github.io/slides-2024-acsos-imageonomics/zebras.jpg">
  
<h1 id="wildlife-behavior-acquisition">Wildlife behavior acquisition</h1>
<p>A paramount tool for <em>ethologists</em> and <em>biologists</em> to gather insights into the nature and inform
<strong>conservation</strong> efforts for <strong>endangered species</strong>.</p>
<ul>
<li>

<span class="fragment ">
  Animal <em>health</em> monitoring
</span>
</li>
<li>

<span class="fragment ">
  <em>Behavioral changes</em> induced by <em>climate change</em> or <em>human activity</em>
</span>
</li>
<li>

<span class="fragment ">
  Current <em>population level</em>
</span>
</li>
<li>

<span class="fragment ">
  Insights into <em>future population levels</em>
</span>
</li>
</ul>
</section>

<section data-noprocess="" data-shortcode-slide="" data-background-image="https://danysk.github.io/slides-2024-acsos-imageonomics/collar.jpg">
  
<div style="
position: fixed;
/* bottom: 0;
right: 0; */
font-size: .3em;
top: -500px;
left: 50%;
transform: translate(-50%, -50%);
">background image: Abujoy, License CC BY-SA</div>
</section>

<section data-noprocess="" data-shortcode-slide="" data-background-image="https://danysk.github.io/slides-2024-acsos-imageonomics/collarblur.jpg">
  
<div style="
position: fixed;
/* bottom: 0;
right: 0; */
font-size: .3em;
top: -500px;
left: 50%;
transform: translate(-50%, -50%);
">background image derived from: Abujoy, License CC BY-SA</div>
</section>

<section data-noprocess="" data-shortcode-slide="" data-background-image="https://danysk.github.io/slides-2024-acsos-imageonomics/collarblur2.jpg">
  
<h1 id="gps-collars">GPS collars</h1>
<ul>
<li><i class="fa-solid fa-check" style="color: green;"></i>
Great position tracking</li>
<li><i class="fa-solid fa-check" style="color: green;"></i>
Possibly equipped with further sensors (temperature, accelerometer…)</li>
<li><i class="fa-solid fa-check" style="color: green;"></i>
Long battery life</li>
<li><i class="fa-solid fa-xmark" style="color: red;"></i>
No video</li>
<li><i class="fa-solid fa-xmark" style="color: red;"></i>
Invasive (requires capture and release) $\Rightarrow$ Limited sample size</li>
</ul>
</section>

<section data-noprocess="" data-shortcode-slide="" data-background-image="https://danysk.github.io/slides-2024-acsos-imageonomics/trap.jpg">
  
<div style="
position: fixed;
/* bottom: 0;
right: 0; */
font-size: .3em;
top: -500px;
left: 0%;
transform: translate(-50%, -50%);
color: white;
">author: Arddu, License CC Attribution 2.0 Generic</div>
</section>

<section data-noprocess="" data-shortcode-slide="" data-background-image="https://danysk.github.io/slides-2024-acsos-imageonomics/trap2.jpg">
  
<div style="
position: fixed;
/* bottom: 0;
right: 0; */
font-size: .3em;
top: -500px;
left: 0%;
transform: translate(-50%, -50%);
color: white;
">author: Winterline, License CC Attribution-Share Alike 3.0 Unported</div>
</section>

<section data-noprocess="" data-shortcode-slide="" data-background-image="https://danysk.github.io/slides-2024-acsos-imageonomics/trap3.jpg">
  
<div style="
position: fixed;
/* bottom: 0;
right: 0; */
font-size: .3em;
top: -500px;
left: 0%;
transform: translate(-50%, -50%);
color: white;
">author: Kalyan Varma, License CC BY-SA</div>
</section>

<section data-noprocess="" data-shortcode-slide="" data-background-image="https://danysk.github.io/slides-2024-acsos-imageonomics/trap3-blur.jpg">
  
</section>

<section data-noprocess="" data-shortcode-slide="" data-background-image="https://danysk.github.io/slides-2024-acsos-imageonomics/trap-setup.jpg">
  
<div style="
position: fixed;
/* bottom: 0;
right: 0; */
font-size: .3em;
top: -500px;
left: 0%;
transform: translate(-50%, -50%);
color: blue;
">author: Prashanthns, License CC BY-SA</div>
</section>

<section data-noprocess="" data-shortcode-slide="" data-background-image="https://danysk.github.io/slides-2024-acsos-imageonomics/trap-setup-blur.jpg">
  
<h1 id="camera-traps">Camera traps</h1>
<ul>
<li><i class="fa-solid fa-check" style="color: green;"></i>
Photos and potentially videos</li>
<li><i class="fa-solid fa-check" style="color: green;"></i>
Non-invasive</li>
<li><i class="fa-solid fa-check" style="color: green;"></i>
Multiple species</li>
<li><i class="fa-solid fa-xmark" style="color: red;"></i>
Static and with limited range</li>
<li><i class="fa-solid fa-xmark" style="color: red;"></i>
False triggers</li>
<li><i class="fa-solid fa-xmark" style="color: red;"></i>
Subject to vandalism and theft</li>
<li><i class="fa-solid fa-xmark" style="color: red;"></i>
Generally fragile (the tiger in the first picture destroyed the camera)</li>
</ul>
</section>

<section data-noprocess="" data-shortcode-slide="" data-background-video="https://danysk.github.io/slides-2024-acsos-imageonomics/nadir.mkv" data-background-video-loop="true" data-background-video-muted="true">
  
</section>

<section data-noprocess="" data-shortcode-slide="" data-background-opacity="0.2" data-background-video="https://danysk.github.io/slides-2024-acsos-imageonomics/nadir.mkv" data-background-video-loop="true" data-background-video-muted="true">
  
<h1 id="fixed-wing-drone-aerial-views">Fixed-wing drone aerial views</h1>
<ul>
<li><i class="fa-solid fa-check" style="color: green;"></i>
<em>Very large area</em> coverage</li>
<li><i class="fa-solid fa-check" style="color: green;"></i>
<em>Long flights</em></li>
<li><i class="fa-solid fa-xmark" style="color: red;"></i>
<strong>Nadir</strong> imagery: good for mapping, bad for <em>individual behavior</em></li>
<li><i class="fa-solid fa-xmark" style="color: red;"></i>
Requires <em>specialized training</em></li>
<li><i class="fa-solid fa-xmark" style="color: red;"></i>
<em>Predefined</em> flight paths</li>
</ul>
</section>

<section data-noprocess="" data-shortcode-slide="" data-background-video="https://danysk.github.io/slides-2024-acsos-imageonomics/nonnadir.mkv" data-background-video-loop="true" data-background-video-muted="true">
  
</section>

<section data-noprocess="" data-shortcode-slide="" data-background-opacity="0.5" data-background-video="https://danysk.github.io/slides-2024-acsos-imageonomics/nonnadir.mkv" data-background-video-loop="true" data-background-video-muted="true">
  
<h2 id="non-nadir-perspective">Non-nadir perspective</h2>
<p><img src="non-nadir-fov.svg" alt=""></p>
</section>

<section data-noprocess="" data-shortcode-slide="" data-background-opacity="0.2" data-background-video="https://danysk.github.io/slides-2024-acsos-imageonomics/nonnadir.mkv" data-background-video-loop="true" data-background-video-muted="true">
  
<h1 id="quadcopters-and-similar-drones">Quadcopters and similar drones</h1>

<div class="container w-100 m-0 p-0">
    <div class="row "><div class="col "><ul>
<li><i class="fa-solid fa-check" style="color: green;"></i>
<em>Large area</em> coverage
<ul>
<li>Although much <em>smaller than fixed-wing</em> drones</li>
</ul>
</li>
<li><i class="fa-solid fa-check" style="color: green;"></i>
<strong>Non-Nadir</strong> view is great for <em>individual behavior</em></li>
<li><i class="fa-solid fa-check" style="color: green;"></i>
<strong>Multiple</strong> drones can get <em>different perspectives</em></li>
<li><i class="fa-solid fa-check" style="color: green;"></i>
<strong>Dynamic trajectories</strong></li>
<li><i class="fa-solid fa-xmark" style="color: red;"></i>
<em>Noise</em> may disturb wildlife</li>
<li><i class="fa-solid fa-xmark" style="color: red;"></i>
Relatively <em>short battery life</em></li>
<li><i class="fa-solid fa-xmark" style="color: red;"></i>
<em>Skilled pilots</em> required
<ul>
<li><i class="fa-solid fa-xmark" style="color: red;"></i>
<strong>Practically impossible</strong> to coordinate multiple drones effectively by hand</li>
</ul>
</li>
</ul>
</div>
<div class="col "><span class="fragment ">
  <p><img src="jenna-drone.jpg" style="height: 15em"><img src="flyingdronefromlandrover.png" style="height: 15em"></p>
</span>
</div>
</div>
</div>


<span class="fragment ">
  <h3 id="rightarrow-multi-drone-coordination">$\Rightarrow$ <strong>Multi-Drone Coordination</strong></h3>
<ul>
<li><em>No</em> need for <em>human pilots</em></li>
<li>Similar to <em>well-known problems in the literature</em>!</li>
</ul>

</span>

</section>

<section data-noprocess="" data-shortcode-slide="" data-background-image="https://danysk.github.io/slides-2024-acsos-imageonomics/zebras.jpg">
  
<h1 id="a-special-omokc">A special OMOkC</h1>
<p>In the <em>Online Multi-Object k-Coverage</em> (<strong>OMOkC</strong>) problem,
dones coordinate to cover each <em>interesting</em> target with at least $k$ points of view.</p>
<video width="1600" height="450" autoplay="" muted="" loop="">
  <source src="omokc.mp4" type="video/mp4">
  <source src="omokc.webm" type="video/webm">
  Your browser does not support the video tag.
</video>
<p>Our problem is a variant of OMOkC, in which:</p>
<ol>
<li>

<span class="fragment ">
  The focus is on animal <em>groups</em> rather than single animals $\Rightarrow$ <strong>Herd tracking</strong>
</span>
</li>
<li>

<span class="fragment ">
  Drones have a <em>blind zone</em> due to their non-nadir point of view $\Rightarrow$ <strong>Blind zone</strong>
</span>
</li>
<li>

<span class="fragment ">
  The <em>position</em> of animals within the Field-of-View dramatically changes the quality of the result $\Rightarrow$ <strong>FoV centrality</strong>
</span>
</li>
<li>

<span class="fragment ">
  The <em>angle</em> at which a subject is being observed matters, lateral views are more informative than frontal ones $\Rightarrow$ <strong>Observation angle</strong>
</span>
</li>
<li>

<span class="fragment ">
  Observers emit <em>noise</em> that may alter the behavior of the observed animals $\Rightarrow$ <strong>Noise pollution</strong>
</span>
</li>
<li>

<span class="fragment ">
  Observation is performed in contexts with <em>limited infrastructure</em> $\Rightarrow$ <strong>Decentralized coordination</strong>
</span>
</li>
</ol>
</section>

<section data-noprocess="" data-shortcode-slide="" data-background-image="https://danysk.github.io/slides-2024-acsos-imageonomics/zebras.jpg">
  
<h1 id="contribution">Contribution</h1>
<h2 id="a-methodology-to-evaluate-the-performance-in-wildlife-video-acquisition">A methodology to evaluate the performance in wildlife video acquisition</h2>
<p>We define <strong>metrics</strong> for:</p>
<ol>
<li>The <em>centrality</em> in the Field-of-View of each camera</li>
<li>The overall <em>angles</em> of observation of each animal</li>
<li>The <em>noise</em> pollution generated by the drones</li>
</ol>
<p>We build simulations based on a novel <em>herd simulation</em> algorithm based on the <a href="https://doi.ieeecomputersociety.org/10.1109/WACVW60836.2024.00011">KABR dataset</a><br>
(Jenna presented the algorithm at SISSY on Monday)</p>


<span class="fragment ">
  <p>$\Rightarrow$ We observe that <em>pre-existing OMOkC algorithms do not perform as well as expected in our context</em>,
and thus we propose to extend the current SOTA with:</p>
<h2 id="an-herd-aware-decentralized-multi-drone-coordination-algorithm">An herd-aware decentralized multi-drone coordination algorithm</h2>

</span>

</section>

<section data-noprocess="" data-shortcode-slide="" data-background-image="https://danysk.github.io/slides-2024-acsos-imageonomics/zebras.jpg">
  
<h1 id="fov-centrality">FoV Centrality</h1>
<ul>
<li>Let $P_c$ be the center of the FoV $\mathcal{V}$ of camera $c$.</li>
<li>Let $F(c)$ be the maximum distance from the center of the FoV</li>
</ul>
<p>then</p>
<p>$F(c) = \max \left| P - P_c \right| ~ \forall P \in \mathcal{V}$.</p>
<ul>
<li>For any camera $c$, <em>$F(c)$</em> represents the <em>worst possible position</em> in its FoV.</li>
<li>For an animal $z$ located in $P_z$, a normalized estimate of <em>how poorly</em> it is positioned in the FoV of $c$ is:
the ratio between its distance to the center and $F(c)$:
$\frac{\left| P_z - P_c \right|}{F(c)}$</li>
<li>The <em>normalized FoV centrality</em> for a target animal $z$ and a drone $c$ is then: $Q(z, c) = 1 - \frac{\left| P_z - P_c \right|}{F(c)}$</li>
<li>Generalized for a set of cameras $C$ observing a target $z$: $\Gamma(z) = \max_{c \in C} Q(z, c)$</li>
</ul>


<span class="fragment ">
  <div class="container w-100 m-0 p-0">
    <div class="row "><div class="col "><h3 id="tldr-the-closer-to-the-center-the-better">TL;DR: the closer to the center, the better</h3>
<ul>
<li>find the <em>worst possible position</em> to be used as <em>bound</em></li>
<li>use that to estimante <em>how good is the <strong>animal</strong> position for <strong>each camera</strong></em></li>
<li><em>for <strong>each animal</strong></em>, <em>consider only the <strong>best camera</strong></em></li>
</ul>
</div>
<div class="col "><p><img src="fov-centrality.svg" alt="fov-centrality"></p>
</div></div>
</div>

</span>

</section>

<section data-noprocess="" data-shortcode-slide="" data-background-image="https://danysk.github.io/slides-2024-acsos-imageonomics/zebras.jpg">
  
<h1 id="observation-angle-body-coverage">Observation angle: body coverage</h1>
<p><strong>Ideas</strong></p>
<ol>
<li>the best observation comes from a perfectly <em>perpendicular angle</em></li>
<li>the “<em>longer</em>” <em>the side</em> of the animal that is being observed, the better the observation
<ul>
<li>that’s why observations from the side are more valuable than frontal or back ones</li>
</ul>
</li>
<li><em>small deviations</em> from perpendicularity are not that bad</li>
</ol>
<p><img src="body-coverage-metric.svg" alt=""></p>
<ol>
<li><em>approximate</em> the animal’s body with a polygon</li>
<li>for each segment $s$ find the camera $c$ observing the segment midpoint from the smallest angle $\alpha_s$: $c$ has the best available view for $s$</li>
<li>normalize $\alpha_s$ in $[0, 1]$ with $\Phi: [-\frac{\pi}{2}, \frac{\pi}{2}]\rightarrow{}[0, 1]$.</li>
<li>use a logistic function to penalize more the <em>extreme</em> angles: $\Phi(x;\mu,\nu)=\left[1+\left(\frac{x(1-\mu)}{\mu(1-x)}\right)^{-\nu}\right]^{-1}, \mu=\frac{1}{2}, \nu=5$</li>
<li>get the <em>observation quality</em> for $s$: $\xi(s) = \Phi\left(\frac{|\alpha_s|}{\frac{\pi}{2}}; \frac{1}{2}, 5\right)$.</li>
<li><em>repeat for every “side”</em> of the animal in $S_z$ to get the <strong>body coverage</strong> $\Diamond(z) = \frac{\sum_{s \in{} S_z} |s| \cdot \xi(s)}{|S_z|}$</li>
</ol>
</section>

<section data-noprocess="" data-shortcode-slide="" data-background-image="https://danysk.github.io/slides-2024-acsos-imageonomics/zebras.jpg">
  
<h1 id="noise-pollution">Noise pollution</h1>
<p>We need the <em>Sound Pressure Level</em> $L_P$ at the position of the animal.</p>
<p>Of course, manufacturers only provide the <em>Sound Power Level</em> $L_W$, a measure of the sound energy emitted by the drone.</p>
<p>To convert into the SPL at distance $r$ from the drone, we need a <em>directivity factor $Q$</em>:
$L_P = L_W - \left| 10 \log_{10} \left(\frac{Q}{4 \pi r^{2}}\right) \right| $</p>
<p>We assume $Q=1$ (<em>spherical propagation</em>), and $r=1m$ (a <em>typical distance</em> at which manifacturer measure the Sound Power Level).</p>
<p>The $L_P$ perceived by an animal $z$ at distance $d$ from the drone
with air attenuation is:
$ L_{P_d}(z) = L_{P}(z) + 20 \log_{10} \left(\frac{r}{d}\right)$</p>
<p>For multiple drones $C$, their contributions sum:
$L_{P_T}(z) = 10 \log_{10} \left(\sum_{c \in{C}} 10^{\frac{L_{P_c}(z)}{10}} \right)$</p>
<p>To <em>normalize</em> in $[0, 1]$, we assume that <em>a noise below $20dB$</em> (~ a ticking watch) can’t be distinguished from the background,
and <em>a noise above $80dB$</em> (~ police car siren) will <em>always</em> disturb the animal.</p>
<p>Since noise is perceived non-linearly, we use a sigmoid with
$\mu=40dB$ (~ <em>refrigerator hum</em>, our proxy for the <em>background noise</em>).</p>
<p>The final <em>normalized noise metric</em> is thus $\rho(z) = \Phi\left(h(L_{P_T}(z)); h(\mu), 4\right)$</p>


<span class="fragment ">
  <h3 id="tldr">TL;DR</h3>
<ul>
<li>we assume noise <strong>propagates in air</strong> without major obstacles or reflections</li>
<li>we set <strong>silence</strong> at the sound of a ticking watch, and <strong>maximum noise</strong> at the level of a police siren</li>
<li>we sum the contribution of <strong>every drone</strong> and consider <strong>non-linear perception</strong></li>
</ul>

</span>

</section>

<section data-noprocess="" data-shortcode-slide="" data-background-image="https://danysk.github.io/slides-2024-acsos-imageonomics/linpro.gif">
  
<div style="position: absolute; left: 0px; top: 0px">
<h1 id="plain-linpro">plain LinPro</h1>
</div>
</section>

<section data-noprocess="" data-shortcode-slide="" data-background-image="https://danysk.github.io/slides-2024-acsos-imageonomics/linpro.gif" data-background-opacity="0.2">
  
<h1 id="herd-sensitive-tracking">Herd-sensitive tracking</h1>
<p>Running state-of-the-art OMOkC algorithms¹ on our setup highlighted some issues:</p>
<ul>
<li>OMOkC algorithms are designed to cover <em>individual</em> targets, not <em>groups</em></li>
<li>Current SOTA algorithms are meant to quickly react to <em>changes in interestingness</em>, but all animals are equally interesting</li>
<li>Usual setups have enough drones to provide $k$ views for each target, but with herds <em>targets largely outnumber</em> drones</li>
</ul>
<h3 id="rightarrow-we-alter-the-general-structure-of-omokc-algorithms-to-track-herd-centroids-instead-of-individual-targets">$\Rightarrow$ We alter the general structure of OMOkC algorithms to track herd centroids instead of individual targets.</h3>
<ol>
<li><strong>Identification and localization</strong>: each drone identifies and localizes the animals in its FoV as best as it can
<ul>
<li>we accept localization and identification errors</li>
</ul>
</li>
<li><strong>Information exchange and consensus</strong>: local information is exchanged among drones to reach consensus on the herd composition,
then each drone, locally, performs a <em>recursive hierarchical agglomerative clustering</em>² to find the <em>herd centroid</em>
<ul>
<li>we accept limited communication ranges and network segmentation</li>
<li>we accept that different drones may have different information and compute different centroids</li>
</ul>
</li>
<li><strong>Prioritization</strong>: we feed the locally-computed herd centroids to the original OMOkC algorithms</li>
</ol>
<span style="text-align: left; font-size: 0.5em; position: absolute; left: 0em; bottom: -10em">
<ol>
<li><a href="https://doi.org/10.1145/3547145">D. Pianini, F. Pettinari, R. Casadei, and L. Esterle, “A collective adaptive approach to decentralised k-coverage in multi-robot systems,” ACM Trans. Auton. Adapt. Syst., vol 17, pp. 4:1–4:39, 2022.</a></li>
<li><a href="https://doi.org/10.1016/0031-3203(79)90049-9">A. Lukasová, “Hierarchical agglomerative clustering procedure,” Pattern Recognit. 11(5-6): 365-381, 1979</a></li>
</ol>
</span>
</section>

<section data-noprocess="" data-shortcode-slide="" data-background-image="https://danysk.github.io/slides-2024-acsos-imageonomics/linproc.gif">
  
<div style="position: absolute; left: 0px; top: 0px">
<h1 id="linpro--clustering">LinPro + clustering</h1>
</div>
</section>

<section data-noprocess="" data-shortcode-slide="" data-background-image="https://danysk.github.io/slides-2024-acsos-imageonomics/linproc.gif" data-background-opacity="0.15">
  
<h1 id="evaluation">Evaluation</h1>
<ul>
<li>Simulation of a <em>2x2km arena</em> realized in Alchemist¹, algorithms written in Protelis²
<ul>
<li>aggregate computing³ worked quite well for the decentralized coordination</li>
</ul>
</li>
<li>video capture <em>session of 30 minutes</em> (to avoid concerns related to battery life)</li>
<li><em>140 grazing zebras</em>, moving at a maximum speed of $2\frac{m}{s}$ split in 2, 4, or 8 separate herds</li>
<li><em>drone-to-herd ratio</em> of 1:1, 2:1, and 3:1.</li>
<li>drones can move at $10\frac{m}{s}$ and have a line-of-sight communication range of $1km$.</li>
<li>Experiments available and reproducible: <a href="https://github.com/nicolasfara/experiments-2024-ACSOS-imageonomics-drones">https://github.com/nicolasfara/experiments-2024-ACSOS-imageonomics-drones</a> <a href="https://zenodo.org/doi/10.5281/zenodo.10931792"><img src="https://zenodo.org/badge/DOI/10.5281/zenodo.10931792.svg" alt="DOI"></a></li>
</ul>
<div id="qrcode0" style="position: absolute; right: 0px; top: 0px"></div>
<script type="text/javascript">
    const qrCode = new QRCodeStyling({
        width: 300,
        height: 300,
        type: "svg",
        data: "https://github.com/nicolasfara/experiments-2024-ACSOS-imageonomics-drones",
        image: "https://danysk.github.io/slides-2024-acsos-imageonomics/drone-svgrepo-com.svg",
        dotsOptions: {
            color: "#0000AA ",
            type: "rounded"
        },
        backgroundOptions: {
            color: "#ffffffff",
        },
        imageOptions: {
            crossOrigin: "anonymous",
            margin: 10
        }
    });
    qrCode.append(document.getElementById("qrcode0"));
    // qrCode.download({ name: "qr", extension: "svg" });
</script>
<span style="text-align: left; font-size: 0.5em; position: absolute; left: 0em; bottom: -15em">
<ol>
<li><img src="https://alchemistsimulator.github.io/images/logo.svg" style="width: 2em; margin-top: 0px; margin-bottom: 0px"> <a href="https://alchemistsimulator.github.io/">https://alchemistsimulator.github.io/</a></li>
<li><img src="https://github.com/Protelis/Protelis.github.io/raw/da40bb7c50d49683ce8ea9d32aab71cf225bd23d/images/2018-03-16-protelis-icon.svg" style="width: 2em; margin-top: 0px; margin-bottom: 0px"> <a href="https://protelis.github.io/">https://protelis.github.io/</a></li>
<li><a href="https://doi.org/10.1109/MC.2015.261">Aggregate Programming for the Internet of Things. Computer 48(9): 22-30 (2015)</a></li>
</ol>
</span>
</section>

<section data-noprocess="" data-shortcode-slide="" data-background-image="https://danysk.github.io/slides-2024-acsos-imageonomics/linproc.gif" data-background-opacity="0.15">
  
<h1 id="overall-results">Overall results</h1>
<p><strong>global metric</strong>, $\nu~\Rightarrow~$ drones per every herd, $\zeta~\Rightarrow~$ herd count</p>
<p><img src="selected_global_metric_by_algorithms.pdf.svg" alt=""></p>
<ul>
<li>Force-Field LinPro+Clustering (<code>ff_linpro_c</code>) is the best across the board</li>
<li>Plain Force-Field LinPro, that outperforms all other algorithms in “classic” OMOkC scenarios, is the <em>worst</em> in our context</li>
<li>The higher the drone:herd ratio, and the more herds, the larger is the gap between <code>ff_linpro_c</code> and the remainder of the algorithms, showing better adaptation</li>
</ul>
</section>

<section data-noprocess="" data-shortcode-slide="" data-background-image="https://danysk.github.io/slides-2024-acsos-imageonomics/linproc.gif" data-background-opacity="0.15">
  
<h1 id="coverage-results">Coverage results</h1>
<p>1-, 2-, and 3-coverage, all algorithms configured to achieve 3-coverage ($k=3$)</p>
<p><img src="k_coverage_by_algorithms_CamHerRatio=2.0_NumberOfHerds=8.0.pdf.svg" alt="">
<br>
<img src="k_coverage_by_algorithms_CamHerRatio=3.0_NumberOfHerds=8.0.pdf.svg" alt=""></p>
<ul>
<li>Force-Field LinPro+Clustering (<code>ff_linpro_c</code>) is the best but for 1-coverage and too few drones</li>
<li>Smooth-Available (<code>sm_av</code>) achieves good 1-coverage, but performance degrades with higher coverages
<ul>
<li>It is likely that <code>ff_linpro_c</code> configured with $k=1$ would perform better</li>
</ul>
</li>
<li>Plain LinPro (<code>ff_linpro</code>) and Neighbor-Broadcast-Received-Calls (<code>bc_re</code>), our baselines, perform consistently poorly</li>
</ul>
</section>

<section data-noprocess="" data-shortcode-slide="" data-background-image="https://danysk.github.io/slides-2024-acsos-imageonomics/linproc.gif" data-background-opacity="0.15">
  
<h1 id="quality-and-noise-results">Quality and noise results</h1>
<p>Geometric mean across all experiments, broken down for each metric</p>
<table>
  <thead>
      <tr>
          <th style="text-align: left">$\Diamond~\Rightarrow$ body coverage</th>
          <th style="text-align: left">$\Gamma~\Rightarrow$ FoV centrality</th>
          <th style="text-align: left">$\rho~\Rightarrow$ noise pollution</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: left"><img src="body-coverage.svg" alt=""></td>
          <td style="text-align: left"><img src="fov-centrality-results.svg" alt=""></td>
          <td style="text-align: left"><img src="noise.svg" alt=""></td>
      </tr>
  </tbody>
</table>
<ul>
<li>LinPro+Clustering (<code>ff_linpro_c</code>) and Smooth-Available (<code>sm_av</code>) are the <em>noisiest</em> because they achieve better <em>coverage</em></li>
<li>Neighbor-Broadcast-Received-Calls (<code>bc_re</code>) tends to over-cover few animals, leading poor centrality and loud noise</li>
</ul>
</section>

<section data-noprocess="" data-shortcode-slide="" data-background-image="https://danysk.github.io/slides-2024-acsos-imageonomics/mixed_herd_screen_shot.jpg" data-background-opacity="0.2">
  
<h1 id="future-work">Future work</h1>
<table>
  <thead>
      <tr>
          <th style="text-align: left">Algorithmic improvements</th>
          <th style="text-align: left">Model improvements</th>
          <th style="text-align: left">Evaluation improvements</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: left">Adaptive clustering threshold</td>
          <td style="text-align: left">Noise-sensitive herds</td>
          <td style="text-align: left">Robustness analysis</td>
      </tr>
      <tr>
          <td style="text-align: left">Learning-based approaches</td>
          <td style="text-align: left">Energy model</td>
          <td style="text-align: left">Network requirement analysis</td>
      </tr>
      <tr>
          <td style="text-align: left">Battery management</td>
          <td style="text-align: left">Multiple species</td>
          <td style="text-align: left">Computational weight analysis</td>
      </tr>
      <tr>
          <td style="text-align: left">Noise-aware optimization</td>
          <td style="text-align: left"></td>
          <td style="text-align: left"></td>
      </tr>
      <tr>
          <td style="text-align: left">Mission-level control</td>
          <td style="text-align: left"></td>
          <td style="text-align: left"></td>
      </tr>
  </tbody>
</table>
</section>

<section data-noprocess="" data-shortcode-slide="" data-background-video="https://danysk.github.io/slides-2024-acsos-imageonomics/nonnadir.mkv" data-background-video-loop="true" data-background-video-muted="true">
  
</section>

  


</div>
      

    </div>
<script type="text/javascript" src="/slides-2024-acsos-imageonomics/reveal-hugo/object-assign.js"></script>

<a href="/slides-2024-acsos-imageonomics/reveal-js/dist/print/" id="print-location" style="display: none;"></a>

<script type="application/json" id="reveal-hugo-site-params">{"custom_theme":"custom-theme.scss","custom_theme_compile":true,"custom_theme_options":{"enablesourcemap":true,"targetpath":"css/custom-theme.css"},"height":"1080","highlight_theme":"solarized-dark","history":true,"mermaid":[{}],"slide_number":true,"theme":"league","transition":"slide","transition_speed":"fast","width":"1920"}</script>
<script type="application/json" id="reveal-hugo-page-params">null</script>

<script src="/slides-2024-acsos-imageonomics/reveal-js/dist/reveal.js"></script>


  
  
  <script type="text/javascript" src="/slides-2024-acsos-imageonomics/reveal-js/plugin/markdown/markdown.js"></script>
  
  <script type="text/javascript" src="/slides-2024-acsos-imageonomics/reveal-js/plugin/highlight/highlight.js"></script>
  
  <script type="text/javascript" src="/slides-2024-acsos-imageonomics/reveal-js/plugin/zoom/zoom.js"></script>
  
  <script type="text/javascript" src="/slides-2024-acsos-imageonomics/reveal-js/plugin/notes/notes.js"></script>
  
  
  <script type="text/javascript" src="/slides-2024-acsos-imageonomics/reveal-js/plugin/notes/notes.js"></script>




<script type="text/javascript">
  
  
  function camelize(map) {
    if (map) {
      Object.keys(map).forEach(function(k) {
        newK = k.replace(/(\_\w)/g, function(m) { return m[1].toUpperCase() });
        if (newK != k) {
          map[newK] = map[k];
          delete map[k];
        }
      });
    }
    return map;
  }

  var revealHugoDefaults = { center: true, controls: true, history: true, progress: true, transition: "slide" };

  var revealHugoPlugins = { 
    plugins: [ RevealMarkdown, RevealHighlight, RevealNotes, RevealZoom ]
   };
  var revealHugoSiteParams = JSON.parse(document.getElementById('reveal-hugo-site-params').innerHTML);
  var revealHugoPageParams = JSON.parse(document.getElementById('reveal-hugo-page-params').innerHTML);
  
  var options = Object.assign({},
    camelize(revealHugoDefaults),
    camelize(revealHugoSiteParams),
    camelize(revealHugoPageParams),
    camelize(revealHugoPlugins));
  Reveal.initialize(options);
</script>






  

  

  

  



    <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']]
      },
      svg: {
        fontCache: 'global'
      }
    };
</script>

<script type="text/javascript" id="MathJax-script" async="" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
</script>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.1/dist/js/bootstrap.bundle.min.js" integrity="sha384-HwwvtgBNo3bZJJLYd8oVXjrBZt8cqVSpeBNS5n7C8IVInixGAoxmnlMuBnhbgrkm" crossorigin="anonymous"></script>

<script>
  if (/.*?(\?|&)print-pdf/.test(window.location.toString())) {
      var ytVideos = document.getElementsByTagName("iframe")
      for (let i = 0; i < ytVideos.length; i++) {
          var videoFrame = ytVideos[i]
          var isYouTube = /^https?:\/\/(www.)youtube\.com\/.*/.test(videoFrame.src)
          if (isYouTube) {
              console.log(`Removing ${videoFrame.src}`)
              var parent = videoFrame.parentElement
              videoFrame.remove()
              var p = document.createElement('p')
              p.append(
                  document.createTextNode(
                      "There was an embedded video here, but it is disabled in the printed version of the slides."
                  )
              )
              p.append(document.createElement('br'))
              p.append(
                  document.createTextNode(
                      `Visit instead ${
                          videoFrame.src
                      } or ${
                          videoFrame.src.replace(
                              /(^https?:\/\/(www.)youtube\.com)\/(embed\/)(\w+).*/,
                              "https://www.youtube.com/watch?v=$4"
                          )
                      }`
                  )
              )
              parent.appendChild(p)
          }
      }
  }
</script>


    
  

</body></html>